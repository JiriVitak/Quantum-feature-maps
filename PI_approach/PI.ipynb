{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa56336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Original shape: (171, 14)\n",
      "Numeric features shape: (171, 13) labels shape: (171,)\n",
      "Reduced feature shape: (171, 8)\n",
      "Computing kernel matrix (this may take time). N = 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [10:03<00:00,  5.07s/it]\n",
      "100%|██████████| 52/52 [02:14<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with path-integral kernel...\n",
      "Path-kernel test acc: 0.6538461538461539\n",
      "Path-kernel test AUC: 0.6924369747899161\n",
      "Training baseline RBF SVM on reduced features...\n",
      "RBF test acc: 0.6538461538461539\n",
      "RBF test AUC: 0.6235294117647059\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# path_kernel_toxicity.py\n",
    "# Requirements: numpy, pandas, scikit-learn, tqdm\n",
    "# Run: python path_kernel_toxicity.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm import trange, tqdm\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "# Kernel hyperparams\n",
    "N_SLICES = 40         # number of discrete time slices in each path (including endpoints)\n",
    "M_SAMPLES = 2000     # Monte Carlo samples per pair (lower for speed; increase for accuracy)\n",
    "BURN_IN = 50         # burn-in steps for each path chain\n",
    "H_BAR = 1.0          # effective Planck constant (tune)\n",
    "MASS = 1.0           # mass parameter in kinetic term\n",
    "DELTA_T = 1.0 / (N_SLICES - 1)  # time step\n",
    "# For speed on laptop, we'll subsample dataset\n",
    "MAX_SAMPLES = 300    # <= number of rows to use (set to None to use all)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "dataset_save = \"Dataset/Toxicity-13F.csv\"\n",
    "\n",
    "# ---------- Preprocess ----------\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(dataset_save)\n",
    "print(\"Original shape:\", df.shape)\n",
    "# Inspect columns: we assume last column is label or there's a 'label' column; adapt as needed.\n",
    "# For this UCI CSV the labels may be in a column named 'toxicity' or similar — try to infer:\n",
    "if 'toxicity' in df.columns:\n",
    "    label_col = 'toxicity'\n",
    "else:\n",
    "    # fallback: try the last column as label\n",
    "    label_col = df.columns[-1]\n",
    "\n",
    "# Drop rows with missing label\n",
    "df = df.dropna(subset=[label_col])\n",
    "# Select numeric columns only for simplicity\n",
    "num_df = df.select_dtypes(include=[np.number]).copy()\n",
    "# remove the label column from features if present\n",
    "if label_col in num_df.columns:\n",
    "    y = num_df[label_col].values\n",
    "    X = num_df.drop(columns=[label_col]).values\n",
    "else:\n",
    "    y = df[label_col].values\n",
    "    # take numeric features from original df\n",
    "    X = num_df.values\n",
    "\n",
    "print(\"Numeric features shape:\", X.shape, \"labels shape:\", y.shape)\n",
    "\n",
    "# Optionally subsample for speed\n",
    "if MAX_SAMPLES is not None and X.shape[0] > MAX_SAMPLES:\n",
    "    idx = np.random.choice(X.shape[0], MAX_SAMPLES, replace=False)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    print(f\"Subsampled to {MAX_SAMPLES} rows for speed.\")\n",
    "\n",
    "# Standardize and PCA to small d\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=min(8, Xs.shape[1]))\n",
    "Xred = pca.fit_transform(Xs)\n",
    "print(\"Reduced feature shape:\", Xred.shape)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xred, y, test_size=0.3, random_state=RANDOM_SEED, stratify=(y if len(np.unique(y))>1 else None))\n",
    "\n",
    "# ---------- Path integral action ----------\n",
    "# We'll map each data vector x -> a potential V(q; x) using radial basis functions.\n",
    "# Precompute centers and widths for phi_j(q).\n",
    "def make_phi_basis(num_basis=8):\n",
    "    centers = np.linspace(-2.0, 2.0, num_basis)\n",
    "    width = 0.6\n",
    "    def phi(q):\n",
    "        # q can be array\n",
    "        vals = np.exp(-((q[:, None] - centers[None, :])**2) / (2*width**2))\n",
    "        return vals  # shape (len(q), num_basis)\n",
    "    return phi, centers\n",
    "\n",
    "phi_func, centers = make_phi_basis(num_basis=Xred.shape[1] + 2)\n",
    "\n",
    "def potential(q, x):\n",
    "    # q: scalar or array. x: feature vector (dim matches number of basis used)\n",
    "    # V(q; x) = base + sum_j x_j * phi_j(q)\n",
    "    phis = np.exp(-((q[:, None] - centers[None, :])**2) / (2 * 0.6**2))  # shape (len(q), B)\n",
    "    # Map x to first B components (pad or slice)\n",
    "    xpad = np.zeros(phis.shape[1])\n",
    "    k = min(len(x), phis.shape[1])\n",
    "    xpad[:k] = x[:k]\n",
    "    V = phis.dot(xpad)\n",
    "    return V  # shape (len(q),)\n",
    "\n",
    "def euclidean_action(path_q, x):\n",
    "    # path_q: array of length N_SLICES, boundary included\n",
    "    # x: data vector for potential\n",
    "    kinetic = 0.0\n",
    "    potential_sum = 0.0\n",
    "    for n in range(1, len(path_q)):\n",
    "        dq = (path_q[n] - path_q[n-1]) / DELTA_T\n",
    "        kinetic += 0.5 * MASS * dq * dq * DELTA_T\n",
    "    # potential evaluated at interior points\n",
    "    qs = np.array(path_q)\n",
    "    Vvals = potential(qs, x)\n",
    "    potential_sum = np.sum(Vvals) * DELTA_T\n",
    "    return kinetic + potential_sum\n",
    "\n",
    "# ---------- Brownian bridge Metropolis sampler ----------\n",
    "def brownian_bridge_proposal(curr_path, sigma=0.5):\n",
    "    # propose by adding Gaussian noise to interior points (not endpoints)\n",
    "    proposal = curr_path.copy()\n",
    "    interior = np.arange(1, len(curr_path) - 1)\n",
    "    proposal[interior] += np.random.normal(scale=sigma, size=interior.shape[0])\n",
    "    return proposal\n",
    "\n",
    "def sample_paths_between(x_from, x_to, M=M_SAMPLES):\n",
    "    # build initial linear bridge between boundary projections\n",
    "    b0 = np.dot(x_from, np.ones_like(centers)[:len(x_from)]) if len(x_from)>0 else 0.0\n",
    "    bT = np.dot(x_to, np.ones_like(centers)[:len(x_to)]) if len(x_to)>0 else 0.0\n",
    "    # Simple boundary projection: take first component of xred as scalar BCs\n",
    "    # Better: learn projection; here we use first feature as scalar BC\n",
    "    try:\n",
    "        b0 = float(x_from[0])\n",
    "        bT = float(x_to[0])\n",
    "    except:\n",
    "        b0 = 0.0; bT = 0.0\n",
    "    path0 = np.linspace(b0, bT, N_SLICES)\n",
    "    curr_path = path0.copy()\n",
    "    curr_S = euclidean_action(curr_path, (x_from + x_to) / 2.0)\n",
    "    weights = []\n",
    "    # Burn-in\n",
    "    for _ in range(BURN_IN):\n",
    "        prop = brownian_bridge_proposal(curr_path, sigma=0.6)\n",
    "        prop[0], prop[-1] = b0, bT\n",
    "        prop_S = euclidean_action(prop, (x_from + x_to) / 2.0)\n",
    "        if np.random.rand() < np.exp(-(prop_S - curr_S) / H_BAR):\n",
    "            curr_path, curr_S = prop, prop_S\n",
    "    # Collect samples\n",
    "    for _ in range(M):\n",
    "        prop = brownian_bridge_proposal(curr_path, sigma=0.4)\n",
    "        prop[0], prop[-1] = b0, bT\n",
    "        prop_S = euclidean_action(prop, (x_from + x_to) / 2.0)\n",
    "        if np.random.rand() < np.exp(-(prop_S - curr_S) / H_BAR):\n",
    "            curr_path, curr_S = prop, prop_S\n",
    "        weights.append(np.exp(-curr_S / H_BAR))\n",
    "    return np.array(weights)\n",
    "\n",
    "# ---------- Build kernel matrix (pairwise) ----------\n",
    "def compute_kernel_matrix(Xdata):\n",
    "    N = Xdata.shape[0]\n",
    "    K = np.zeros((N, N))\n",
    "    print(\"Computing kernel matrix (this may take time). N =\", N)\n",
    "    for i in tqdm(range(N)):\n",
    "        for j in range(i, N):\n",
    "            w = sample_paths_between(Xdata[i], Xdata[j], M=M_SAMPLES)\n",
    "            K_ij = np.mean(w)\n",
    "            K[i, j] = K_ij\n",
    "            K[j, i] = K_ij\n",
    "    # Normalize kernel (optional)\n",
    "    # Make PSD by adding small jitter\n",
    "    K += 1e-8 * np.eye(N)\n",
    "    return K\n",
    "\n",
    "# For speed, compute kernel on a limited subset of train data\n",
    "TRAIN_SUB = min(120, X_train.shape[0])\n",
    "TEST_SUB = min(60, X_test.shape[0])\n",
    "\n",
    "Xtr_small = X_train[:TRAIN_SUB]\n",
    "ytr_small = y_train[:TRAIN_SUB]\n",
    "Xte_small = X_test[:TEST_SUB]\n",
    "yte_small = y_test[:TEST_SUB]\n",
    "\n",
    "K_train = compute_kernel_matrix(Xtr_small)\n",
    "# For test kernel we compute cross-kernel between test and train\n",
    "def compute_cross_kernel(Xa, Xb):\n",
    "    A = Xa.shape[0]; B = Xb.shape[0]\n",
    "    K = np.zeros((A, B))\n",
    "    for i in tqdm(range(A)):\n",
    "        for j in range(B):\n",
    "            w = sample_paths_between(Xa[i], Xb[j], M=int(M_SAMPLES/4))  # fewer samples for cross\n",
    "            K[i, j] = np.mean(w)\n",
    "    return K\n",
    "\n",
    "K_test = compute_cross_kernel(Xte_small, Xtr_small)\n",
    "\n",
    "# ---------- Train SVM with precomputed kernel ----------\n",
    "print(\"Training SVM with path-integral kernel...\")\n",
    "clf = SVC(kernel='precomputed', probability=True)\n",
    "clf.fit(K_train, ytr_small)\n",
    "y_pred = clf.predict(K_test)\n",
    "y_score = clf.decision_function(K_test) if hasattr(clf, \"decision_function\") else clf.predict_proba(K_test)[:, 1]\n",
    "print(\"Path-kernel test acc:\", accuracy_score(yte_small, y_pred))\n",
    "try:\n",
    "    print(\"Path-kernel test AUC:\", roc_auc_score(yte_small, y_score))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ---------- Baseline: RBF on PCA features ----------\n",
    "print(\"Training baseline RBF SVM on reduced features...\")\n",
    "rbf = SVC(kernel='rbf', probability=True)\n",
    "rbf.fit(Xtr_small, ytr_small)\n",
    "y_pred_rbf = rbf.predict(Xte_small)\n",
    "y_score_rbf = rbf.decision_function(Xte_small)\n",
    "print(\"RBF test acc:\", accuracy_score(yte_small, y_pred_rbf))\n",
    "try:\n",
    "    print(\"RBF test AUC:\", roc_auc_score(yte_small, y_score_rbf))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1035f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"K_train.npy\", K_train)\n",
    "np.save(\"K_test.npy\", K_test)\n",
    "np.save(\"y_train.npy\", ytr_small)   # or your training labels variable\n",
    "np.save(\"y_test.npy\", yte_small)\n",
    "# optionally save reduced features for RBF baseline:\n",
    "np.save(\"X_train_red.npy\", Xtr_small)\n",
    "np.save(\"X_test_red.npy\", Xte_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "724c5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed kernel and labels from disk...\n",
      "Shapes: K_train (119, 119) K_test (52, 119) y_train (119,) y_test (52,)\n",
      "\n",
      "--- Kernel normalization & centering ---\n",
      "Train kernel - min/max/mean before: 1.2857643360120744e-32 70.19064776471305 0.7661542881461306\n",
      "Train kernel normalized - min/max/mean after: 1.2764776744356858e-32 2.611212156780805 0.4667110179603607\n",
      "\n",
      "--- GridSearchCV for SVM C (5-fold) ---\n",
      "Best params: {'C': 0.01} best CV score (AUC): 0.6566964285714285\n",
      "\n",
      "--- Final training with best C on full train set ---\n",
      "Final path-kernel test acc: 0.6731, AUC: 0.6773\n",
      "\n",
      "--- Kernel diagnostics ---\n",
      "Kernel alignment (centered) with labels: 0.011850028735979968\n",
      "Top 10 eigenvalue fractions: [0.54181511 0.31416716 0.14418591 0.08323632 0.04523573 0.03235044\n",
      " 0.02047996 0.01784213 0.01572442 0.01012907]\n",
      "Cumulative explained by top eigenvalues (first 10): [0.54181511 0.85598228 1.00016819 1.08340451 1.12864024 1.16099068\n",
      " 1.18147064 1.19931278 1.2150372  1.22516627]\n",
      "\n",
      "--- Bootstrap AUC (retrain on bootstrap samples) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstraps: 100%|██████████| 200/200 [00:00<00:00, 444.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap AUC mean: 0.5512, 95% CI: [0.3966, 0.6808] (n_boot=200)\n",
      "\n",
      "--- If you have reduced features X_train/X_test, you can run an RBF baseline for comparison ---\n",
      "RBF best params: {'C': 0.1, 'gamma': 1.0} CV AUC: 0.7377232142857142\n",
      "RBF test acc: 0.6730769230769231 RBF test AUC: 0.5915966386554622\n",
      "\n",
      "Saved centered kernels to K_train_centered.npy and K_test_centered.npy.\n",
      "Diagnostics complete.\n"
     ]
    }
   ],
   "source": [
    "# path_kernel_diagnostics.py\n",
    "# Purpose: normalize/center path kernel, grid-search SVM C, compute alignment, eigenspectrum, and bootstrap AUC CI.\n",
    "# Assumes you already computed K_train, K_test and have y_train_subset / y_test_subset available.\n",
    "# If K_train.npy and K_test.npy exist, script will load them. Otherwise it will call compute_kernel_matrix/compute_cross_kernel\n",
    "# from your existing code (you can paste the compute_kernel_matrix & compute_cross_kernel functions or set RECOMPUTE=True).\n",
    "#\n",
    "# Requirements: numpy, scipy, scikit-learn, joblib, tqdm, pandas\n",
    "# Run: python path_kernel_diagnostics.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import pairwise\n",
    "import scipy.linalg as la\n",
    "import joblib\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---------- User-editable paths ----------\n",
    "K_TRAIN_PATH = \"K_train.npy\"\n",
    "K_TEST_PATH = \"K_test.npy\"\n",
    "Y_TRAIN_PATH = \"y_train.npy\"\n",
    "Y_TEST_PATH = \"y_test.npy\"\n",
    "\n",
    "RECOMPUTE = False  # set True if you want this script to recompute kernels (expensive)\n",
    "# If RECOMPUTE=True, ensure compute_kernel_matrix(Xtr_small) and compute_cross_kernel(Xte_small,Xtr_small)\n",
    "# are available in scope (paste from your previous script or import them).\n",
    "\n",
    "# ---------- Utility functions ----------\n",
    "def center_kernel(K):\n",
    "    \"\"\"Center kernel matrix K (NxN).\"\"\"\n",
    "    n = K.shape[0]\n",
    "    one = np.ones((n, n)) / n\n",
    "    return K - one.dot(K) - K.dot(one) + one.dot(K).dot(one)\n",
    "\n",
    "def normalize_kernel(K):\n",
    "    \"\"\"Normalize kernel to cosine (K_ij -> K_ij / sqrt(K_ii K_jj)).\"\"\"\n",
    "    diag = np.diag(K).copy()\n",
    "    # numerical safety\n",
    "    diag[diag <= 0] = 1e-12\n",
    "    D = np.sqrt(diag)\n",
    "    K_norm = K / np.outer(D, D)\n",
    "    return K_norm\n",
    "\n",
    "def kernel_alignment(K, y):\n",
    "    \"\"\"Compute (centered) kernel alignment with label kernel L = y y^T (y in {-1,1}).\"\"\"\n",
    "    Kc = center_kernel(K)\n",
    "    # convert labels to +/-1 (if 0/1 present)\n",
    "    yv = np.array(y)\n",
    "    if set(np.unique(yv)) == {0,1}:\n",
    "        yv = 2*yv - 1\n",
    "    L = np.outer(yv, yv)\n",
    "    # center L as well\n",
    "    Lc = center_kernel(L)\n",
    "    num = np.sum(Kc * Lc)\n",
    "    den = np.sqrt(np.sum(Kc * Kc) * np.sum(Lc * Lc))\n",
    "    return float(num / den) if den > 0 else 0.0\n",
    "\n",
    "def kernel_eigenspectrum(K, top_k=10):\n",
    "    \"\"\"Return sorted eigenvalues (desc) and fraction explained by top components.\"\"\"\n",
    "    # Use symmetric eigensolver\n",
    "    vals = la.eigvalsh(K)\n",
    "    vals_sorted = np.sort(vals)[::-1]\n",
    "    total = np.sum(vals_sorted)\n",
    "    rel = vals_sorted[:top_k] / total if total != 0 else vals_sorted[:top_k]\n",
    "    explained = np.cumsum(vals_sorted) / total if total != 0 else np.cumsum(vals_sorted)\n",
    "    return vals_sorted, rel, explained\n",
    "\n",
    "# Bootstrapping AUC for a precomputed kernel (train/test)\n",
    "def bootstrap_auc_precomputed(K_train, K_test, y_train, y_test, C=1.0, n_boot=200, seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Bootstrap resampling of training set to estimate distribution of test AUC.\n",
    "    We resample with replacement from the training indices, retrain SVM, evaluate on fixed test set.\n",
    "    Returns list of AUCs.\n",
    "    \"\"\"\n",
    "    n_train = K_train.shape[0]\n",
    "    aucs = []\n",
    "    rng = np.random.RandomState(seed)\n",
    "    svc = SVC(kernel='precomputed', probability=True, C=C)\n",
    "    for b in trange(n_boot, desc=\"Bootstraps\"):\n",
    "        idx = rng.randint(0, n_train, size=n_train)  # bootstrap indices\n",
    "        # build bootstrap training kernel (submatrix)\n",
    "        Kb = K_train[np.ix_(idx, idx)]\n",
    "        yb = y_train[idx]\n",
    "        # train on bootstrap set\n",
    "        try:\n",
    "            svc.fit(Kb, yb)\n",
    "        except Exception as e:\n",
    "            print(\"SVM fit error on bootstrap, skipping:\", e)\n",
    "            continue\n",
    "        # build cross-kernel between test and bootstrap-train (test rows correspond to K_test columns)\n",
    "        # But SVC expects columns corresponding to training set ordering. We must compute K_test_boot where\n",
    "        # K_test_boot[i, j] = K_test[i, original_index_of_bootstrap_j]\n",
    "        Kt_boot = K_test[:, idx]\n",
    "        # predict and evaluate\n",
    "        try:\n",
    "            scores = svc.decision_function(Kt_boot)\n",
    "        except Exception:\n",
    "            scores = svc.predict_proba(Kt_boot)[:, 1]\n",
    "        auc = roc_auc_score(y_test, scores)\n",
    "        aucs.append(auc)\n",
    "    return aucs\n",
    "\n",
    "# ---------- Load data / kernels ----------\n",
    "if os.path.exists(K_TRAIN_PATH) and os.path.exists(K_TEST_PATH) and os.path.exists(Y_TRAIN_PATH) and os.path.exists(Y_TEST_PATH) and not RECOMPUTE:\n",
    "    print(\"Loading precomputed kernel and labels from disk...\")\n",
    "    K_train = np.load(K_TRAIN_PATH)\n",
    "    K_test = np.load(K_TEST_PATH)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # after loading y_train and y_test (possibly with allow_pickle=True)\n",
    "    y_train_raw = np.load(Y_TRAIN_PATH, allow_pickle=True)\n",
    "    y_test_raw  = np.load(Y_TEST_PATH, allow_pickle=True)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    # fit only on train labels (safe); transform both\n",
    "    y_train = le.fit_transform(y_train_raw)\n",
    "    y_test  = le.transform(y_test_raw)\n",
    "else:\n",
    "    if RECOMPUTE:\n",
    "        # Placeholder: user should paste in compute_kernel_matrix & compute_cross_kernel functions\n",
    "        print(\"RECOMPUTE=True but compute_kernel_matrix not found in this script. Please paste your kernel-building functions or precompute kernels and save them to files.\")\n",
    "        raise RuntimeError(\"Kernel recomputation requested but compute functions are not available here.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Precomputed kernel files not found. Either set RECOMPUTE=True and provide compute functions, or run your path-kernel script and save K_train.npy, K_test.npy, y_train.npy, y_test.npy.\")\n",
    "\n",
    "print(\"Shapes: K_train\", K_train.shape, \"K_test\", K_test.shape, \"y_train\", y_train.shape, \"y_test\", y_test.shape)\n",
    "\n",
    "# ---------- Normalize, center, and compare scaling ----------\n",
    "print(\"\\n--- Kernel normalization & centering ---\")\n",
    "K_train_norm = normalize_kernel(K_train)\n",
    "K_train_centered = center_kernel(K_train_norm)\n",
    "# For test kernel we must normalize with training diagonal entries:\n",
    "# Normalize K_test such that K_test_norm[i,j] = K_test[i,j] / sqrt(K_ii_train * K_jj_train)\n",
    "train_diag = np.sqrt(np.diag(K_train))\n",
    "train_diag[train_diag <= 0] = 1e-12\n",
    "K_test_norm = K_test / np.outer(np.ones(K_test.shape[0]), train_diag)\n",
    "\n",
    "# Center training kernel for SVM CV (note: centering precomputed kernels for SVM is common practice,\n",
    "# but when using precomputed kernel with scikit-learn you must center explicitly before passing.)\n",
    "K_train_used = K_train_centered.copy()\n",
    "# For SVM precomputed with gridsearch, we'll use K_train_used.\n",
    "\n",
    "print(\"Train kernel - min/max/mean before:\", np.min(K_train), np.max(K_train), np.mean(K_train))\n",
    "print(\"Train kernel normalized - min/max/mean after:\", np.min(K_train_norm), np.max(K_train_norm), np.mean(K_train_norm))\n",
    "\n",
    "# ---------- Grid search for SVM C (5-fold CV on precomputed kernel) ----------\n",
    "print(\"\\n--- GridSearchCV for SVM C (5-fold) ---\")\n",
    "param_grid = {'C': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "svc = SVC(kernel='precomputed')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "grid = GridSearchCV(svc, param_grid=param_grid, cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "# GridSearchCV expects features shaped (n_samples, n_features) - for precomputed kernels it expects the kernel matrix\n",
    "# as X when fitting; pass the centered kernel.\n",
    "grid.fit(K_train_used, y_train)\n",
    "print(\"Best params:\", grid.best_params_, \"best CV score (AUC):\", grid.best_score_)\n",
    "bestC = grid.best_params_['C']\n",
    "\n",
    "# ---------- Train final SVM on centered kernel and evaluate on test set ----------\n",
    "print(\"\\n--- Final training with best C on full train set ---\")\n",
    "clf = SVC(kernel='precomputed', C=bestC, probability=True)\n",
    "clf.fit(K_train_used, y_train)\n",
    "\n",
    "# Build test kernel matrix to match centered training set normalization/centering.\n",
    "# Note: proper kernel centering for cross-kernel K_test requires:\n",
    "# Kc_test = K_test_norm - 1_n_test 1_n_train^T K_train_norm - K_test_norm 1_n_train 1_n_train^T + 1_n_test 1_n_train^T K_train_norm 1_n_train 1_n_train^T\n",
    "# But a simpler approximate approach used in practice: center K_test using train mean vectors.\n",
    "def center_cross_kernel(K_cross, K_train):\n",
    "    # K_cross: (n_test, n_train)\n",
    "    n_train = K_train.shape[0]\n",
    "    one_train = np.ones((n_train, n_train)) / n_train\n",
    "    K_train_mean_rows = np.mean(K_train, axis=0)  # vector length n_train\n",
    "    K_train_mean_all = np.mean(K_train)\n",
    "    K_cross_centered = K_cross - np.mean(K_cross, axis=1)[:, None] - K_train_mean_rows[None, :] + K_train_mean_all\n",
    "    return K_cross_centered\n",
    "\n",
    "K_test_used = center_cross_kernel(K_test_norm, K_train_norm)\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    test_scores = clf.decision_function(K_test_used)\n",
    "except Exception:\n",
    "    test_scores = clf.predict_proba(K_test_used)[:, 1]\n",
    "test_pred = clf.predict(K_test_used)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_auc = roc_auc_score(y_test, test_scores)\n",
    "print(\"Final path-kernel test acc: {:.4f}, AUC: {:.4f}\".format(test_acc, test_auc))\n",
    "\n",
    "# ---------- Kernel diagnostics: alignment and eigenspectrum ----------\n",
    "print(\"\\n--- Kernel diagnostics ---\")\n",
    "align = kernel_alignment(K_train, y_train)\n",
    "print(\"Kernel alignment (centered) with labels:\", align)\n",
    "vals_sorted, top_rel, explained = kernel_eigenspectrum(K_train_used, top_k=10)\n",
    "print(\"Top 10 eigenvalue fractions:\", top_rel)\n",
    "print(\"Cumulative explained by top eigenvalues (first 10):\", explained[:10])\n",
    "\n",
    "# ---------- Bootstrap AUC CI ----------\n",
    "print(\"\\n--- Bootstrap AUC (retrain on bootstrap samples) ---\")\n",
    "n_boot = 200\n",
    "aucs = bootstrap_auc_precomputed(K_train_used, K_test_used, y_train, y_test, C=bestC, n_boot=n_boot, seed=RANDOM_SEED)\n",
    "aucs = np.array(aucs)\n",
    "mean_auc = np.mean(aucs)\n",
    "ci_low = np.percentile(aucs, 2.5)\n",
    "ci_high = np.percentile(aucs, 97.5)\n",
    "print(f\"Bootstrap AUC mean: {mean_auc:.4f}, 95% CI: [{ci_low:.4f}, {ci_high:.4f}] (n_boot={len(aucs)})\")\n",
    "\n",
    "# ---------- Compare to RBF baseline (optional if you have X_train/X_test) ----------\n",
    "print(\"\\n--- If you have reduced features X_train/X_test, you can run an RBF baseline for comparison ---\")\n",
    "# If user has saved reduced features arrays, load them and compute RBF\n",
    "if os.path.exists(\"X_train_red.npy\") and os.path.exists(\"X_test_red.npy\"):\n",
    "    Xtr = np.load(\"X_train_red.npy\")\n",
    "    Xte = np.load(\"X_test_red.npy\")\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # simple grid search on gamma and C (small grid)\n",
    "    param_grid_rbf = {'C':[0.1,1,10], 'gamma':[0.01, 0.1, 1.0]}\n",
    "    rbf = SVC(kernel='rbf', probability=True)\n",
    "    gs_rbf = GridSearchCV(rbf, param_grid=param_grid_rbf, cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "    gs_rbf.fit(Xtr, y_train)\n",
    "    print(\"RBF best params:\", gs_rbf.best_params_, \"CV AUC:\", gs_rbf.best_score_)\n",
    "    rbf_best = gs_rbf.best_estimator_\n",
    "    y_pred_rbf = rbf_best.predict(Xte)\n",
    "    try:\n",
    "        scores_rbf = rbf_best.decision_function(Xte)\n",
    "    except:\n",
    "        scores_rbf = rbf_best.predict_proba(Xte)[:,1]\n",
    "    print(\"RBF test acc:\", accuracy_score(y_test, y_pred_rbf), \"RBF test AUC:\", roc_auc_score(y_test, scores_rbf))\n",
    "else:\n",
    "    print(\"No X_train_red.npy / X_test_red.npy found — skip RBF auto-eval. If you want this, save reduced PCA features as X_train_red.npy and X_test_red.npy.\")\n",
    "\n",
    "# Save outputs for future use\n",
    "np.save(\"K_train_centered.npy\", K_train_used)\n",
    "np.save(\"K_test_centered.npy\", K_test_used)\n",
    "print(\"\\nSaved centered kernels to K_train_centered.npy and K_test_centered.npy.\")\n",
    "print(\"Diagnostics complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406db613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best Result by test_auc ===\n",
      "H_BAR    : 0.5\n",
      "MASS     : 0.5\n",
      "N_SLICES : 12\n",
      "M_SAMPLES: 200\n",
      "test_auc : 0.8854166666666667\n",
      "alignment: 0.0515012447334938\n",
      "bestCV_AUC: 0.6733333333333333\n",
      "bestC    : 10.0\n",
      "label    : H0.5_M0.5_S12_Ms200\n",
      "\n",
      "\n",
      "=== Top 3 Results by test_auc ===\n",
      "     H_BAR  MASS  N_SLICES  M_SAMPLES  test_auc  alignment  bestCV_AUC  bestC  \\\n",
      "53     0.5   0.5        12        200  0.885417   0.051501    0.673333  10.00   \n",
      "125    0.8   0.5        12        200  0.843750   0.027572    0.643333   0.01   \n",
      "116    0.7   2.0         8        200  0.833333   0.033307    0.246667  10.00   \n",
      "\n",
      "                   label  \n",
      "53   H0.5_M0.5_S12_Ms200  \n",
      "125  H0.8_M0.5_S12_Ms200  \n",
      "116   H0.7_M2.0_S8_Ms200  \n",
      "\n",
      "\n",
      "=== Top 3 Results by Combined Score (0.75*test_auc + 0.25*alignment) ===\n",
      "     H_BAR  MASS  N_SLICES  M_SAMPLES  test_auc  alignment  combined_score  \\\n",
      "53     0.5   0.5        12        200  0.885417   0.051501        0.882388   \n",
      "153    0.9   1.0        10        200  0.708333   0.131988        0.830000   \n",
      "150    0.9   1.0         6        200  0.791667   0.072085        0.822466   \n",
      "\n",
      "     bestC                label  \n",
      "53    10.0  H0.5_M0.5_S12_Ms200  \n",
      "153    1.0  H0.9_M1.0_S10_Ms200  \n",
      "150   10.0   H0.9_M1.0_S6_Ms200  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "csv_path = \"sweep_results/sweep_summary.csv\"  # path to your sweep CSV\n",
    "top_k = 3                                    # how many top results to show\n",
    "\n",
    "# ------------------ Load CSV ------------------\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure 'label' column exists\n",
    "if 'label' not in df.columns:\n",
    "    df['label'] = df.index.astype(str)\n",
    "\n",
    "# ------------------ Best single result by test_auc ------------------\n",
    "best_row = df.loc[df['test_auc'].idxmax()]\n",
    "\n",
    "print(\"=== Best Result by test_auc ===\")\n",
    "print(f\"H_BAR    : {best_row['H_BAR']}\")\n",
    "print(f\"MASS     : {best_row['MASS']}\")\n",
    "print(f\"N_SLICES : {best_row['N_SLICES']}\")\n",
    "print(f\"M_SAMPLES: {best_row.get('M_SAMPLES', 'N/A')}\")\n",
    "print(f\"test_auc : {best_row['test_auc']}\")\n",
    "print(f\"alignment: {best_row['alignment']}\")\n",
    "print(f\"bestCV_AUC: {best_row.get('bestCV_AUC', 'N/A')}\")\n",
    "print(f\"bestC    : {best_row.get('bestC', 'N/A')}\")\n",
    "print(f\"label    : {best_row.get('label', 'N/A')}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# ------------------ Top-K by test_auc ------------------\n",
    "top_auc = df.sort_values(by='test_auc', ascending=False).head(top_k)\n",
    "print(f\"=== Top {top_k} Results by test_auc ===\")\n",
    "print(top_auc[['H_BAR','MASS','N_SLICES','M_SAMPLES','test_auc','alignment','bestCV_AUC','bestC','label']])\n",
    "print(\"\\n\")\n",
    "\n",
    "# ------------------ Combined score (test_auc + alignment) ------------------\n",
    "def combined_score(df, w_auc=0.75, w_align=0.25):\n",
    "    s = df.copy()\n",
    "    # fill NaNs\n",
    "    s['test_auc_filled'] = s['test_auc'].fillna(-999)\n",
    "    s['alignment_filled'] = s['alignment'].fillna(-999)\n",
    "    # min-max scale to [0,1]\n",
    "    def minmax_col(arr):\n",
    "        a = np.array(arr, dtype=float)\n",
    "        amin = np.nanmin(a)\n",
    "        amax = np.nanmax(a)\n",
    "        if amax <= amin:\n",
    "            return np.zeros_like(a)\n",
    "        return (a - amin) / (amax - amin)\n",
    "    s['auc_scaled'] = minmax_col(s['test_auc_filled'])\n",
    "    s['align_scaled'] = minmax_col(s['alignment_filled'])\n",
    "    s['combined_score'] = w_auc * s['auc_scaled'] + w_align * s['align_scaled']\n",
    "    return s\n",
    "\n",
    "df_combined = combined_score(df)\n",
    "top_combined = df_combined.sort_values(by='combined_score', ascending=False).head(top_k)\n",
    "\n",
    "print(f\"=== Top {top_k} Results by Combined Score (0.75*test_auc + 0.25*alignment) ===\")\n",
    "print(top_combined[['H_BAR','MASS','N_SLICES','M_SAMPLES','test_auc','alignment','combined_score','bestC','label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62375667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_train:\n",
      " [[1.         0.20628742 0.07631253 0.17544535 0.08758749]\n",
      " [0.20628742 1.         0.00742173 0.20372521 0.20779223]\n",
      " [0.07631253 0.00742173 1.         0.01793638 0.01669364]\n",
      " [0.17544535 0.20372521 0.01793638 1.         0.08076974]\n",
      " [0.08758749 0.20779223 0.01669364 0.08076974 1.        ]]\n",
      "K_test:\n",
      " [[0.1003442  0.3334711  0.02144049 0.13818103 0.57093522]\n",
      " [0.01816577 0.16751916 0.07865527 0.1443786  0.34000603]]\n",
      "Predictions: [0 0]\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# ------------------ Hyperparameters ------------------\n",
    "N_QUBITS = 4             # number of qubits (can reduce by PCA on features)\n",
    "N_SLICES = 12            # discretized time slices (from classical sweep)\n",
    "H_BAR = 0.5              # effective Planck constant\n",
    "MASS = 0.5               # mass parameter\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------ Device ------------------\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "\n",
    "# ------------------ Feature map ------------------\n",
    "def kinetic_layer():\n",
    "    \"\"\"Single-qubit rotations representing kinetic term.\"\"\"\n",
    "    for w in range(N_QUBITS):\n",
    "        qml.RX(H_BAR / MASS, wires=w)\n",
    "\n",
    "def potential_layer(x):\n",
    "    \"\"\"Multi-qubit rotations representing potential term (couplings).\"\"\"\n",
    "    for i, j in product(range(N_QUBITS), repeat=2):\n",
    "        if i < j:\n",
    "            # simple two-qubit rotation encoding interactions\n",
    "            qml.CRZ((x[i] * x[j]) * H_BAR, wires=[i, j])\n",
    "\n",
    "def path_integral_feature_map(x):\n",
    "    \"\"\"Encode input x into a path-integral quantum state.\"\"\"\n",
    "    x = x[:N_QUBITS]  # truncate if needed\n",
    "    for w in range(N_QUBITS):\n",
    "        qml.RY(x[w], wires=w)  # angle encoding of features\n",
    "    \n",
    "    for _ in range(N_SLICES):\n",
    "        kinetic_layer()\n",
    "        potential_layer(x)\n",
    "\n",
    "# ------------------ Kernel evaluation ------------------\n",
    "@qml.qnode(dev)\n",
    "def kernel_circuit(x1, x2):\n",
    "    # Prepare first state |psi(x1)>\n",
    "    path_integral_feature_map(x1)\n",
    "    # Inverse of second state |psi(x2)>^\\dagger\n",
    "    qml.adjoint(path_integral_feature_map)(x2)\n",
    "    return qml.probs(wires=range(N_QUBITS))\n",
    "\n",
    "def compute_kernel_matrix(X1, X2):\n",
    "    \"\"\"Compute kernel matrix K_ij = |<psi(x_i)|psi(x_j)>|^2\"\"\"\n",
    "    N1 = len(X1)\n",
    "    N2 = len(X2)\n",
    "    K = np.zeros((N1, N2))\n",
    "    for i, x_i in enumerate(X1):\n",
    "        for j, x_j in enumerate(X2):\n",
    "            probs = kernel_circuit(x_i, x_j)\n",
    "            K[i, j] = probs[0]  # probability of |00...0> state\n",
    "    return K\n",
    "\n",
    "# ------------------ Example usage ------------------\n",
    "# Small random dataset (replace with your real features)\n",
    "X_train = np.random.rand(5, N_QUBITS) * np.pi\n",
    "X_test  = np.random.rand(2, N_QUBITS) * np.pi\n",
    "\n",
    "K_train = compute_kernel_matrix(X_train, X_train)\n",
    "K_test  = compute_kernel_matrix(X_test, X_train)\n",
    "\n",
    "print(\"K_train:\\n\", K_train)\n",
    "print(\"K_test:\\n\", K_test)\n",
    "\n",
    "# Now K_train and K_test can be fed to a classical SVM:\n",
    "from sklearn.svm import SVC\n",
    "y_train = np.array([0, 1, 0, 1, 0])\n",
    "y_test  = np.array([1, 0])\n",
    "\n",
    "clf = SVC(kernel=\"precomputed\", C=10.0)\n",
    "clf.fit(K_train, y_train)\n",
    "y_pred = clf.predict(K_test)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a9f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DT_venv)",
   "language": "python",
   "name": "dt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
